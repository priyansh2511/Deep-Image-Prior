{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "super_resolution (1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install utils\n",
        "!pip3 install Pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-NNnofNSDsi",
        "outputId": "65faa218-4be8-4080-ffee-f99171b3f6f0"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: utils in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Lambda, Conv2D, BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import concatenate\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
        "from PIL import Image, ImageFilter\n",
        "# from utils import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.metrics import mse\n",
        "from keras.backend import set_session\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as im\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.compat.v1 as tf"
      ],
      "metadata": {
        "id": "Dt8Oqd1CRMIB"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(img):\n",
        "    img_cropped = img_to_array(img)\n",
        "    return (np.expand_dims(img_cropped, axis=0).astype('float32'))/255\n",
        "\n",
        "\n",
        "def postprocess(img):\n",
        "    if len(img.shape) != 2:\n",
        "      img = tf.keras.utils.array_to_img(img)\n",
        "      return img\n",
        "    else:\n",
        "      img = np.expand_dims(img, axis=-1)\n",
        "      img = tf.keras.utils.array_to_img(img)\n",
        "      return img\n",
        "\n",
        "\n",
        "def crop_image(img, d=32):\n",
        "    x = img.size[0] - img.size[0] % d\n",
        "    y = img.size[1] - img.size[1] % d\n",
        "\n",
        "    dim = []\n",
        "    dim.append(int((img.size[0] - x) / 2))\n",
        "    dim.append(int((img.size[1] - y) / 2))\n",
        "    dim.append(int((img.size[0] + x) / 2))\n",
        "    dim.append(int((img.size[1] + y) / 2))\n",
        "\n",
        "    img_cropped = img.crop(dim)\n",
        "    return img_cropped\n",
        "\n",
        "   \n",
        "def lanczos2_kernel(factor):\n",
        "    a = 2\n",
        "    phase = 0.5\n",
        "    kernel_size = 4 * factor + 2\n",
        "\n",
        "    if phase == 0.5:\n",
        "        kernel = np.zeros([kernel_size - 2, kernel_size - 2])\n",
        "\n",
        "    size1 = kernel.shape[0] + 1\n",
        "    size2 = kernel.shape[1] + 1\n",
        "    center = kernel_size  / 2.\n",
        "\n",
        "    for i in range(1, size1):\n",
        "        for j in range(1, size2):\n",
        "            if phase != 0.5:\n",
        "                di = abs(i - center) / factor\n",
        "                dj = abs(j - center) / factor\n",
        "            else:\n",
        "                di = abs(i + 0.5 - center) / factor\n",
        "                dj = abs(j + 0.5 - center) / factor\n",
        "\n",
        "            val = 1\n",
        "            if di != 0:\n",
        "                val = np.sin(np.pi * di / a) * np.sin(np.pi * di) * val\n",
        "                val = (val * a) / (np.pi * np.pi * di * di)\n",
        "\n",
        "            if dj != 0:\n",
        "                val = np.sin(np.pi * dj / a) * np.sin(np.pi * dj) * val\n",
        "                val = (val * a) / (np.pi * np.pi * dj * dj)\n",
        "\n",
        "            kernel[i - 1][j - 1] = val\n",
        "\n",
        "    kernel = kernel / kernel.sum()\n",
        "\n",
        "    return kernel\n",
        "\n",
        "\n",
        "\n",
        "def Lanczos2Conv2D(x, channel, factor=4, name=None):\n",
        "    kernel = lanczos2_kernel(factor)\n",
        "    weights = np.zeros((kernel.shape[0], kernel.shape[1], channel, channel))\n",
        "    for i in range(channel):\n",
        "        weights[:, :, i, i] = kernel\n",
        "    pad_size = int((kernel.shape[0] - 1) / 2.)\n",
        "    x = tf.pad(x, paddings=[[0, 0], [pad_size, pad_size], [pad_size, pad_size], [0, 0]], mode='REFLECT')\n",
        "    downsampling = Conv2D(channel, kernel.shape[0], strides=factor, use_bias=False)\n",
        "    x = downsampling(x)\n",
        "    downsampling.set_weights([weights])\n",
        "    downsampling.trainable = False\n",
        "    return x"
      ],
      "metadata": {
        "id": "MXbzG_KSRZZQ"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Hourglass:\n",
        "    def __init__(self, input_size, input_channel, output_channel, nu,\n",
        "                  nd, ns, ku,kd, ks,upsample='nearest', use_bias=True):\n",
        "        self.input_size = input_size\n",
        "        self.input_channel = input_channel\n",
        "        self.output_channel = output_channel\n",
        "        self.nu = nu\n",
        "        self.nd = nd\n",
        "        self.ns = ns\n",
        "        self.ku = ku\n",
        "        self.kd = kd\n",
        "        self.ks = ks\n",
        "        self.upsample = upsample\n",
        "        self.use_bias = True\n",
        "\n",
        "    def upsamples(self, x, size=2):\n",
        "        index = x.shape\n",
        "        new_w = int(round(index[1] * size))\n",
        "        new_h = int(round(index[2] * size))\n",
        "        return tf.image.resize(x, [new_w, new_h], method=tf.image.ResizeMethod.BILINEAR);\n",
        "\n",
        "    def down(self, x, n, k, use_bias=True):\n",
        "        pad_size = int((k - 1) / 2)\n",
        "        x = tf.pad(x, paddings=[[0, 0], [pad_size, pad_size], [pad_size, pad_size], [0, 0]], mode='REFLECT')\n",
        "        x = Conv2D(n, k, strides=2, use_bias=use_bias)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(0.2)(x)\n",
        "\n",
        "        x = tf.pad(x, paddings=[[0, 0], [pad_size, pad_size], [pad_size, pad_size], [0, 0]], mode='REFLECT')\n",
        "        x = Conv2D(n, k, strides=1, use_bias=use_bias)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(0.2)(x)\n",
        "        return x\n",
        "\n",
        "    def skip(self, x, n, k, use_bias=True):\n",
        "        pad_size = int((k - 1) / 2)\n",
        "        x = tf.pad(x, paddings=[[0, 0], [pad_size, pad_size], [pad_size, pad_size], [0, 0]], mode='REFLECT')\n",
        "        x = Conv2D(n, k, strides=1, use_bias=use_bias)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(0.2)(x)\n",
        "        return x\n",
        "\n",
        "    def up(self, x, n, k, upsample_mode, use_bias=True):\n",
        "        x = BatchNormalization()(x)\n",
        "        pad_size = int((k - 1) / 2)\n",
        "        x = tf.pad(x, paddings=[[0, 0], [pad_size, pad_size], [pad_size, pad_size], [0, 0]], mode='REFLECT')\n",
        "        x = Conv2D(n, k, strides=1, use_bias=use_bias)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(0.2)(x)\n",
        "\n",
        "        x = Conv2D(n, 1, strides=1, use_bias=use_bias)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU(0.2)(x)\n",
        "\n",
        "        x = self.upsamples(x, 2)\n",
        "        return x\n",
        "\n",
        "    def model_structure(self):\n",
        "        w, h = self.input_size\n",
        "        input = Input((w, h, self.input_channel))\n",
        "        x = input\n",
        "        n = 5\n",
        "        skips = []\n",
        "        for i in range(n):\n",
        "            x = self.down(x, self.nd[i], self.kd[i], self.use_bias)\n",
        "            if self.ns[i] != 0:\n",
        "                skip = self.skip(x, self.ns[i], self.ks[i], self.use_bias)\n",
        "                skips.append(skip)\n",
        "\n",
        "        x = self.up(x, self.nu[n - 1], self.ku[n - 1], self.upsample, self.use_bias)\n",
        "        skips = skips[::-1]\n",
        "        for i in range(n - 1):\n",
        "            j = (n - 1)- i \n",
        "            if i != 0:\n",
        "                x = concatenate([x, skips[i+1]], axis=-1)\n",
        "            x = self.up(x, self.nu[j-1], self.ku[j-1],self.upsample, self.use_bias)\n",
        "\n",
        "        output = Conv2D(self.output_channel, 1, activation='sigmoid')(x)\n",
        "\n",
        "        model = Model(inputs=input, outputs=output)\n",
        "        return model\n",
        "\n"
      ],
      "metadata": {
        "id": "tiXJk1N2giKN"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5T1tDxOQx4X"
      },
      "outputs": [],
      "source": [
        "\n",
        "method = 'random'\n",
        "input_channel = 32\n",
        "lr = 0.01\n",
        "\n",
        "factor = 8\n",
        "if factor == 4:\n",
        "    num_iter = 4000\n",
        "    sigma = 1. / 30\n",
        "elif factor == 8:\n",
        "    num_iter = 6000\n",
        "    sigma = 1. / 20\n",
        "\n",
        "image_path = 'zebra_GT.png'\n",
        "\n",
        "### load data\n",
        "hr_img = load_data(image_path)\n",
        "hr_img = crop_image(hr_img)\n",
        "lr_img = low_resolution(hr_img, factor)\n",
        "hr_x = preprocess(hr_img)\n",
        "lr_x = preprocess(lr_img)\n",
        "postprocess(hr_x[0]).save('hr_ori.png')\n",
        "postprocess(lr_x[0]).save('lr_ori.png')\n",
        "\n",
        "### bicubic, sharpened bicubic and nearest\n",
        "lr_img.resize(hr_img.size, Image.BICUBIC).save('hr_bicubic.png')\n",
        "lr_img.resize(hr_img.size, Image.NEAREST).save('hr_nearest.png')\n",
        "lr_img.resize(hr_img.size, Image.BICUBIC).filter(ImageFilter.UnsharpMask()).save( 'hr_sharpened.png')\n",
        "\n",
        "### build code z\n",
        "b, w, h, c = hr_x.shape\n",
        "\n",
        "z = np.random.uniform(0, 0.1, size=(1,w,h,input_channel))\n",
        "# z = make_noise(method, input_channel, (w, h))          \n",
        "\n",
        "hg = Hourglass((w, h), input_channel, c,\n",
        "                  nu=[128, 128, 128, 128, 128],\n",
        "                  nd=[128, 128, 128, 128, 128],\n",
        "                  ns=[4, 4, 4, 4, 128],\n",
        "                  ku=[3, 3, 3, 3, 3],\n",
        "                  kd=[3, 3, 3, 3, 3],\n",
        "                  ks=[1, 1, 1, 1, 1],\n",
        "                  upsample='bilinear'\n",
        "                  )\n",
        "\n",
        "\n",
        "g = hg.model_structure()\n",
        "input = g.input\n",
        "x = g.output\n",
        "output = Lanczos2Conv2D(x, c, factor)\n",
        "model = Model(inputs=input, outputs=output, name='g_trainer')\n",
        "\n",
        "model.compile(optimizer=Adam(lr=lr), loss=mse)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "for i in range(num_iter + 1):\n",
        "    # loss = model.train_on_batch(add_noise(z, sigma), lr_x)\n",
        "\n",
        "    temp = z + np.random.normal(0, sigma, size = z.shape)\n",
        "    loss = model.train_on_batch(temp, lr_x)\n",
        "    losses.append(loss)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print('iter %d loss %f' % (i, loss))\n",
        "        y = g.predict_on_batch(z)\n",
        "        postprocess(y[0]).save('%d.png' % i)\n",
        "\n",
        "model.save('model.h5')\n",
        "\n",
        "del losses[-1]\n",
        "plt.plot(list(range(num_iter)), losses)\n",
        "plt.savefig('loss.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPeS8mSeJRZs",
        "outputId": "4d129016-c7c1-4608-f3ef-5a157c280b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0 loss 0.076034\n",
            "iter 100 loss 0.014982\n",
            "iter 200 loss 0.005821\n",
            "iter 300 loss 0.002958\n",
            "iter 400 loss 0.002259\n",
            "iter 500 loss 0.001720\n",
            "iter 600 loss 0.001423\n",
            "iter 700 loss 0.001220\n"
          ]
        }
      ]
    }
  ]
}